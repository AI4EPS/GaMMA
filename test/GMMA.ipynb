{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "after-carol",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T03:08:40.993793Z",
     "iopub.status.busy": "2021-05-22T03:08:40.993501Z",
     "iopub.status.idle": "2021-05-22T03:08:43.511479Z",
     "shell.execute_reply": "2021-05-22T03:08:43.510891Z",
     "shell.execute_reply.started": "2021-05-22T03:08:40.993768Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from gmma import mixture\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN \n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "to_seconds = lambda t: t.timestamp(tz=\"UTC\")\n",
    "from_seconds = lambda t: pd.Timestamp.utcfromtimestamp(t).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "# to_seconds = lambda t: datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\").timestamp()\n",
    "# from_seconds = lambda t: [datetime.utcfromtimestamp(x).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] for x in t]\n",
    "\n",
    "def plot_assoc(time_pick, loc_pick, center, pred, eq_idx, dims, bounds, time_range, dt=0.01):\n",
    "    t0 = time_pick[:,0].min()\n",
    "    plt.figure(figsize=(time_range/25, 3))\n",
    "    num = 0\n",
    "    for i in np.argsort(center[:, len(dims)]):\n",
    "        if i in eq_idx:\n",
    "            plt.scatter((time_pick[pred==i,0]-t0), loc_pick[pred==i,1], s=24, marker='o', color=f\"C{num}\", facecolor='none')\n",
    "#             plt.scatter((center[i, len(dims)]-t0), center[i, 1], color=f\"C{num}\", s=4**center[i, len(dims)+1]*3, marker=\"P\")\n",
    "            plt.scatter((center[i, len(dims)]-t0), center[i, 1], color=f\"C{num}\", s=200, marker=\"P\", edgecolor=\"k\")\n",
    "            num += 1\n",
    "        else:\n",
    "            plt.scatter((time_pick[pred==i,0]-t0), loc_pick[pred==i,1], s=24, marker='o', color=\"grey\", alpha=0.5)\n",
    "    \n",
    "    plt.xlim(-8, time_range+3)\n",
    "#     plt.ylim(bounds[1])\n",
    "    plt.ylabel(\"Y (km)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    xlim = plt.xlim()\n",
    "    if xlim[1] > 80:\n",
    "        plt.scatter(xlim[0]-10, 0, s=48, color=\"C0\",  marker=\"o\", facecolors='none', label=\"Associated picks\")\n",
    "        plt.scatter(xlim[0]-10, 0, s=100, color=\"C0\", marker=\"P\", edgecolor=\"k\", label=\"Earthquakes\")\n",
    "        plt.legend(fontsize=\"small\")\n",
    "    try:\n",
    "        plt.savefig((f\"figures_tmp/{from_seconds(t0)}.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.savefig((f\"figures_tmp/{from_seconds(t0)}.pdf\"), bbox_inches=\"tight\")\n",
    "    except:\n",
    "        os.mkdir((\"figures_tmp\"))\n",
    "        plt.savefig((f\"figures_tmp/{from_seconds(t0)}.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "        plt.savefig((f\"figures_tmp/{from_seconds(t0)}.pdf\"), bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def convert_picks_csv(picks, stations, config):\n",
    "    t = picks[\"timestamp\"].apply(lambda x: x.timestamp()).to_numpy()\n",
    "    a = picks[\"amp\"].apply(lambda x: np.log10(x*1e2)).to_numpy()\n",
    "    data = np.stack([t, a]).T\n",
    "    meta = pd.merge(stations, picks[\"id\"], on=\"id\")\n",
    "    locs = meta[config[\"dims\"]].to_numpy()\n",
    "    phase_type = picks[\"type\"].apply(lambda x: x.lower()).to_numpy()\n",
    "    phase_weight = picks[\"prob\"].to_numpy()[:,np.newaxis]\n",
    "    return data, locs, phase_type, phase_weight\n",
    "\n",
    "def association(data, locs, phase_type, phase_weight, num_sta, pick_idx, event_idx0, config, pbar=None):\n",
    "\n",
    "    db = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(np.hstack([data[:,0:1], locs[:,:2]/6.0]))#.fit(data[:,0:1])\n",
    "    labels = db.labels_\n",
    "    unique_labels = set(labels)\n",
    "    events = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "\n",
    "    assignment = []\n",
    "    for k in unique_labels:\n",
    "        if k == -1:\n",
    "            continue\n",
    "\n",
    "        class_mask = (labels == k)\n",
    "        data_ = data[class_mask]\n",
    "        locs_ = locs[class_mask]\n",
    "        phase_type_ = phase_type[class_mask]\n",
    "        phase_weight_ = phase_weight[class_mask]\n",
    "        pick_idx_ = pick_idx[class_mask]\n",
    "        \n",
    "        if len(pick_idx_) <  config[\"min_picks_per_eq\"]:\n",
    "            continue\n",
    "\n",
    "        if pbar is not None:\n",
    "            pbar.set_description(f\"Process {len(data_)} picks\")\n",
    "        \n",
    "        time_range = max(data_[:,0].max() - data_[:,0].min(), 1)\n",
    "        \n",
    "        num_event_loc_init = 5\n",
    "        num_event_init = min(max(int(len(data_)/min(num_sta, 20) * config[\"oversample_factor\"]), 1), len(data)//num_event_loc_init)\n",
    "        x0, xn = config[\"x(km)\"]\n",
    "        y0, yn = config[\"y(km)\"]\n",
    "        x1, y1 = np.mean(config[\"x(km)\"]), np.mean(config[\"y(km)\"])\n",
    "        event_loc_init = [((x0+x1)/2, (y0+y1)/2), ((x0+x1)/2, (yn+y1)/2), ((xn+x1)/2, (y0+y1)/2), ((xn+x1)/2, (yn+y1)/2), (x1, y1)]\n",
    "        num_event_time_init = max(num_event_init//num_event_loc_init, 1)\n",
    "        centers_init = np.vstack([np.vstack([np.ones(num_event_time_init) * x, \n",
    "                                             np.ones(num_event_time_init) * y,\n",
    "                                             np.zeros(num_event_time_init),\n",
    "                                             np.linspace(data_[:,0].min()-0.1*time_range, data_[:,0].max()+0.1*time_range, num_event_time_init)]).T\n",
    "                                  for x, y in event_loc_init])\n",
    "\n",
    "#         num_event_init = min(max(int(len(data_)/min(num_sta, 20) * config[\"oversample_factor\"]), 1), len(data))\n",
    "#         centers_init = np.vstack([np.ones(num_event_init)*np.mean(stations[\"x(km)\"]),\n",
    "#                                   np.ones(num_event_init)*np.mean(stations[\"y(km)\"]),\n",
    "#                                   np.zeros(num_event_init),\n",
    "#                                   np.linspace(data_[:,0].min()-0.1*time_range, data_[:,0].max()+0.1*time_range, num_event_init)]).T\n",
    "        \n",
    "        mean_precision_prior = 0.1/time_range\n",
    "        if not config[\"use_amplitude\"]:\n",
    "            covariance_prior = np.array([[1]]) * 5\n",
    "            data = data[:,0:1]\n",
    "        else:\n",
    "            covariance_prior = np.array([[1,0],[0,1]]) * 5\n",
    "\n",
    "        gmm = mixture.BayesianGaussianMixture(n_components=len(centers_init), \n",
    "                                              weight_concentration_prior=1/len(centers_init),\n",
    "                                              mean_precision_prior=mean_precision_prior,\n",
    "                                              covariance_prior=covariance_prior,\n",
    "                                              init_params=\"centers\",\n",
    "                                              centers_init=centers_init.copy(), \n",
    "                                              station_locs=locs_, \n",
    "                                              phase_type=phase_type_, \n",
    "                                              phase_weight=phase_weight_,\n",
    "                                              loss_type=\"l1\",\n",
    "                                              bounds=config[\"bfgs_bounds\"],\n",
    "                                              max_covar=20**2,\n",
    "                                              ).fit(data_) \n",
    "\n",
    "        pred = gmm.predict(data_) \n",
    "        prob_matrix = gmm.predict_proba(data_)\n",
    "        prob_eq = prob_matrix.mean(axis=0)\n",
    "#             prob = prob_matrix[range(len(data_)), pred]\n",
    "#             score = gmm.score(data_)\n",
    "#             score_sample = gmm.score_samples(data_)\n",
    "        prob = np.exp(gmm.score_samples(data_))\n",
    "\n",
    "        idx = np.array([True if len(data_[pred==i, 0]) >= config[\"min_picks_per_eq\"] else False for i in range(len(prob_eq))]) #& (prob_eq > 1/num_event) #& (sigma_eq[:, 0,0] < 40)\n",
    "\n",
    "        time = gmm.centers_[idx, len(config[\"dims\"])]\n",
    "        loc = gmm.centers_[idx, :len(config[\"dims\"])]\n",
    "        if config[\"use_amplitude\"]:\n",
    "            mag = gmm.centers_[idx, len(config[\"dims\"])+1]\n",
    "        sigma_eq = gmm.covariances_[idx,...]\n",
    "\n",
    "        for i in range(len(time)):\n",
    "            tmp = {\"time(s)\": time[i],\n",
    "                   \"magnitude\": mag[i],\n",
    "                   \"sigma\": sigma_eq[i].tolist()}\n",
    "            for j, k in enumerate(config[\"dims\"]):\n",
    "                tmp[k] = loc[i][j]\n",
    "            events.append(tmp)\n",
    "\n",
    "        for i in range(len(pick_idx_)):\n",
    "            assignment.append((pick_idx_[i], pred[i]+event_idx0, prob[i]))\n",
    "\n",
    "        event_idx0 += len(time)\n",
    "        \n",
    "#         plot_assoc(data_, locs_, gmm.centers_, pred, np.arange(len(idx))[idx], config[\"dims\"], config[\"bfgs_bounds\"], time_range)\n",
    "\n",
    "    return events, assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-pathology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T03:08:43.518177Z",
     "iopub.status.busy": "2021-05-22T03:08:43.517940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 95 picks:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:  {'center': (-117.504, 35.705), 'xlim_degree': [-118.004, -117.004], 'ylim_degree': [35.205, 36.205], 'degree2km': 111.19492474777779, 'starttime': datetime.datetime(2019, 7, 4, 17, 0), 'endtime': datetime.datetime(2019, 7, 5, 0, 0), 'networks': ['*'], 'channels': 'HH*,BH*,EH*,HN*', 'client': 'SCEDC', 'dims': ['x(km)', 'y(km)', 'z(km)'], 'use_dbscan': True, 'use_amplitude': True, 'x(km)': array([-55.59746237,  55.59746237]), 'y(km)': array([-55.59746237,  55.59746237]), 'z(km)': (0, 20), 'bfgs_bounds': ((-56.597462373888895, 56.597462373888895), (-56.597462373888895, 56.597462373888895), (0, 21), (None, None)), 'dbscan_eps': 6, 'dbscan_min_samples': 3, 'min_picks_per_eq': 10, 'oversample_factor': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 76 picks:  29%|██▊       | 2/7 [05:36<13:30, 162.17s/it]  "
     ]
    }
   ],
   "source": [
    "catalog_dir = os.path.join(\"./\")\n",
    "if not os.path.exists(catalog_dir):\n",
    "    os.makedirs(catalog_dir)\n",
    "config_pkl = \"config.pkl\"\n",
    "station_csv = \"stations.csv\"\n",
    "pick_json = \"picks.json\"\n",
    "catalog_csv = \"catalog.csv\"\n",
    "picks_csv = \"picks_gmma.csv\"\n",
    "\n",
    "with open(config_pkl, \"rb\") as fp:\n",
    "    config = pickle.load(fp)\n",
    "\n",
    "## read picks\n",
    "picks = pd.read_json(pick_json)\n",
    "picks[\"time_idx\"] = picks[\"timestamp\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H\")) ## process by hours\n",
    "\n",
    "## read stations\n",
    "stations = pd.read_csv(station_csv, delimiter=\"\\t\")\n",
    "stations = stations.rename(columns={\"station\":\"id\"})\n",
    "stations[\"x(km)\"] = stations[\"longitude\"].apply(lambda x: (x - config[\"center\"][0])*config[\"degree2km\"])\n",
    "stations[\"y(km)\"] = stations[\"latitude\"].apply(lambda x: (x - config[\"center\"][1])*config[\"degree2km\"])\n",
    "stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x/1e3)\n",
    "\n",
    "### setting GMMA configs\n",
    "config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\n",
    "config[\"use_dbscan\"] = True\n",
    "config[\"use_amplitude\"] = True\n",
    "config[\"x(km)\"] = (np.array(config[\"xlim_degree\"])-np.array(config[\"center\"][0]))*config[\"degree2km\"]\n",
    "config[\"y(km)\"] = (np.array(config[\"ylim_degree\"])-np.array(config[\"center\"][1]))*config[\"degree2km\"]\n",
    "config[\"z(km)\"] = (0, 20)\n",
    "# DBSCAN\n",
    "config[\"bfgs_bounds\"] = ((config[\"x(km)\"][0]-1, config[\"x(km)\"][1]+1), #x\n",
    "                        (config[\"y(km)\"][0]-1, config[\"y(km)\"][1]+1), #y\n",
    "                        (0, config[\"z(km)\"][1]+1), #x\n",
    "                        (None, None)) #t\n",
    "config[\"dbscan_eps\"] = min(np.sqrt((stations[\"x(km)\"].max()-stations[\"x(km)\"].min())**2 +\n",
    "                                   (stations[\"y(km)\"].max()-stations[\"y(km)\"].min())**2)/(6.0/1.75), 6) #s\n",
    "config[\"dbscan_min_samples\"] = min(len(stations), 3)\n",
    "# Filtering\n",
    "config[\"min_picks_per_eq\"] = min(len(stations)//2, 10)\n",
    "config[\"oversample_factor\"] = min(len(stations)//2, 10)\n",
    "print(\"Config: \", config)\n",
    "\n",
    "## run GMMA association\n",
    "catalogs = []\n",
    "pbar = tqdm(sorted(list(set(picks[\"time_idx\"]))))\n",
    "event_idx0 = 0 ## current earthquake index\n",
    "assignments = []\n",
    "if (len(picks) > 0) and (len(picks) < 5000):\n",
    "    data, locs, phase_type, phase_weight = convert_picks_csv(picks, stations, config)\n",
    "    catalogs, assignments = association(data, locs, phase_type, phase_weight, len(stations), picks.index.to_numpy(), event_idx0, config, pbar)\n",
    "else:\n",
    "    for i, hour in enumerate(pbar):\n",
    "        picks_ = picks[picks[\"time_idx\"] == hour]\n",
    "        if len(picks_) == 0:\n",
    "            continue\n",
    "        data, locs, phase_type, phase_weight = convert_picks_csv(picks_, stations, config)\n",
    "        catalog, assign = association(data, locs, phase_type, phase_weight, len(stations), picks_.index.to_numpy(), event_idx0, config, pbar)\n",
    "        event_idx0 += len(catalog)\n",
    "        catalogs.extend(catalog)\n",
    "        assignments.extend(assign)\n",
    "\n",
    "## create catalog\n",
    "catalogs = pd.DataFrame(catalogs, columns=[\"time(s)\"]+config[\"dims\"]+[\"magnitude\", \"sigma\"])\n",
    "catalogs[\"time\"] = catalogs[\"time(s)\"].apply(lambda x: from_seconds(x))\n",
    "catalogs[\"longitude\"] = catalogs[\"x(km)\"].apply(lambda x: x/config[\"degree2km\"] + config[\"center\"][0])\n",
    "catalogs[\"latitude\"] = catalogs[\"y(km)\"].apply(lambda x: x/config[\"degree2km\"] + config[\"center\"][1])\n",
    "catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x*1e3)\n",
    "catalogs[\"event_idx\"] = range(event_idx0)\n",
    "if config[\"use_amplitude\"]:\n",
    "    catalogs[\"covariance\"] = catalogs[\"sigma\"].apply(lambda x: f\"{x[0][0]:.3f},{x[1][1]:.3f},{x[0][1]:.3f}\")\n",
    "else:\n",
    "    catalogs[\"covariance\"] = catalogs[\"sigma\"].apply(lambda x: f\"{x[0][0]:.3f}\")\n",
    "with open(catalog_csv, 'w') as fp:\n",
    "    catalogs.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                    float_format=\"%.3f\",\n",
    "                    date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                    columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\", \"covariance\", \"event_idx\"])\n",
    "\n",
    "## add assignment to picks\n",
    "assignments = pd.DataFrame(assignments, columns=[\"pick_idx\", \"event_idx\", \"prob_gmma\"])\n",
    "picks = picks.join(assignments.set_index(\"pick_idx\")).fillna(-1).astype({'event_idx': int})\n",
    "with open(picks_csv, 'w') as fp:\n",
    "    picks.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                    date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                    columns=[\"id\", \"timestamp\", \"type\", \"prob\", \"amp\", \"event_idx\", \"prob_gmma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label=\"GMMA\"\n",
    "catalog_label=\"SCSN\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.pkl\", \"rb\") as fp:\n",
    "    config = pickle.load(fp)\n",
    "    \n",
    "stations = pd.read_csv((\"stations.csv\"), delimiter=\"\\t\")\n",
    "events = pd.read_csv((\"events.csv\"), delimiter=\"\\t\")\n",
    "events[\"time\"] = events[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "\n",
    "catalog = pd.read_csv((\"catalog.csv\"), delimiter=\"\\t\")\n",
    "catalog[\"time\"] = catalog[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "catalog[\"covariance\"] = catalog[\"covariance\"].apply(lambda x: [float(i) for i  in x.split(\",\")])\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(catalog[\"time\"], range=(config[\"starttime\"], config[\"endtime\"]), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{result_label}: {len(catalog['time'])}\")\n",
    "plt.hist(events[\"time\"], range=(config[\"starttime\"], config[\"endtime\"]), bins=24, edgecolor=\"k\", alpha=1.0, linewidth=0.5, label=f\"{catalog_label}: {len(events['time'])}\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.gca().autoscale(enable=True, axis='x', tight=True)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.legend()\n",
    "plt.savefig(\"earthquake_number.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"earthquake_number.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([1.5,1]))\n",
    "box = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "text_loc = [0.05, 0.92]\n",
    "grd = fig.add_gridspec(ncols=2, nrows=2, width_ratios=[1.5, 1], height_ratios=[1,1])\n",
    "fig.add_subplot(grd[:, 0])\n",
    "plt.plot(catalog[\"longitude\"], catalog[\"latitude\"], '.',markersize=2, alpha=1.0)\n",
    "plt.plot(events[\"longitude\"], events[\"latitude\"], '.', markersize=2, alpha=0.6)\n",
    "plt.axis(\"scaled\")\n",
    "plt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27]))\n",
    "plt.ylim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27]))\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(config[\"xlim_degree\"][0]-10, config[\"ylim_degree\"][0]-10, '.', markersize=10, label=f\"{result_label}\", rasterized=True)\n",
    "plt.plot(config[\"xlim_degree\"][0]-10, config[\"ylim_degree\"][0]-10, '.', markersize=10, label=f\"{catalog_label}\", rasterized=True)\n",
    "plt.plot(stations[\"longitude\"], stations[\"latitude\"], 'k^', markersize=5, alpha=0.7, label=\"Stations\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "\n",
    "fig.add_subplot(grd[0, 1])\n",
    "plt.plot(catalog[\"longitude\"], catalog[\"depth(m)\"]/1e3, '.', markersize=2, alpha=1.0, rasterized=True)\n",
    "plt.plot(events[\"longitude\"], events[\"depth(m)\"]/1e3, '.', markersize=2, alpha=0.6, rasterized=True)\n",
    "# plt.axis(\"scaled\")\n",
    "plt.xlim(np.array(config[\"xlim_degree\"])+np.array([0.2,-0.27]))\n",
    "plt.ylim([0,21])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Depth (km)\")\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(config[\"xlim_degree\"][0]-10, 31, '.', markersize=10, label=f\"{result_label}\")\n",
    "plt.plot(31, 31, '.', markersize=10, label=f\"{catalog_label}\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "\n",
    "fig.add_subplot(grd[1, 1])\n",
    "plt.plot(catalog[\"latitude\"], catalog[\"depth(m)\"]/1e3, '.', markersize=2, alpha=1.0, rasterized=True)\n",
    "plt.plot(events[\"latitude\"], events[\"depth(m)\"]/1e3, '.', markersize=2, alpha=0.6, rasterized=True)\n",
    "# plt.axis(\"scaled\")\n",
    "plt.xlim(np.array(config[\"ylim_degree\"])+np.array([0.2,-0.27]))\n",
    "plt.ylim([0,21])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Depth (km)\")\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(config[\"ylim_degree\"][0]-10, 31, '.', markersize=10, label=f\"{result_label}\")\n",
    "plt.plot(31, 31, '.', markersize=10, label=f\"{catalog_label}\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "plt.savefig(\"earthquake_location.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"earthquake_location.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(catalog[\"magnitude\"], range=(-1., events[\"magnitude\"].max()), bins=25, alpha=1.0,  edgecolor=\"k\", linewidth=0.5, label=f\"{result_label}: {len(catalog['magnitude'])}\")\n",
    "plt.hist(events[\"magnitude\"], range=(-1., events[\"magnitude\"].max()), bins=25, alpha=0.6,  edgecolor=\"k\", linewidth=0.5, label=f\"{catalog_label}: {len(events['magnitude'])}\")\n",
    "plt.legend()\n",
    "# plt.figure()\n",
    "plt.xlim([-1,events[\"magnitude\"].max()])\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.gca().set_yscale('log')\n",
    "plt.savefig(\"earthquake_magnitude_frequency.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"earthquake_magnitude_frequency.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(catalog[\"time\"], catalog[\"magnitude\"], '.', markersize=5, alpha=1.0, rasterized=True)\n",
    "plt.plot(events[\"time\"], events[\"magnitude\"], '.', markersize=5, alpha=0.8, rasterized=True)\n",
    "plt.xlim(config[\"starttime\"], config[\"endtime\"])\n",
    "ylim = plt.ylim()\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.xlabel(\"Date\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.plot(config[\"starttime\"], -10, '.', markersize=15, alpha=1.0, label=f\"{result_label}: {len(catalog['magnitude'])}\")\n",
    "plt.plot(config[\"starttime\"], -10, '.', markersize=15, alpha=1.0, label=f\"{catalog_label}: {len(events['magnitude'])}\")\n",
    "plt.legend()\n",
    "plt.ylim(ylim)\n",
    "plt.grid()\n",
    "plt.savefig(\"earthquake_magnitude_time.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"earthquake_magnitude_time.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.array(catalog[\"covariance\"].to_list())\n",
    "\n",
    "fig = plt.figure(figsize=plt.rcParams[\"figure.figsize\"]*np.array([0.8,1.1]))\n",
    "box = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "text_loc = [0.05, 0.90]\n",
    "plt.subplot(311)\n",
    "plt.plot(catalog[\"time\"], covariance[:,0], '.', markersize=3.0, label=\"Travel-time\")\n",
    "plt.ylim([0, 3])\n",
    "plt.ylabel(r\"$\\Sigma_{11}$ (s)$^2$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.text(text_loc[0], text_loc[1], '(i)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "plt.subplot(312)\n",
    "plt.plot(catalog[\"time\"], covariance[:,1], '.', markersize=3.0, label=\"Amplitude\")\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(r\"$\\Sigma_{22}$ ($\\log10$ m/s)$^2$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.text(text_loc[0], text_loc[1], '(ii)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "plt.subplot(313)\n",
    "plt.plot(catalog[\"time\"], covariance[:,2], '.', markersize=3.0, label=\"Travel-time vs. Amplitude\")\n",
    "plt.ylabel(r\"$\\Sigma_{12}$\")\n",
    "plt.ylim([-0.5, 0.5])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.text(text_loc[0], text_loc[1], '(iii)', horizontalalignment='left', verticalalignment=\"top\", \n",
    "         transform=plt.gca().transAxes, fontsize=\"large\", fontweight=\"normal\", bbox=box)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d:%H'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "# plt.suptitle(r\"Covariance Matrix ($\\Sigma$) Coefficients\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().align_labels()\n",
    "plt.savefig(\"covariance.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"covariance.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-kelly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-salon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
